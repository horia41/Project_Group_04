{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Iterative structured pruning",
   "id": "4c5fa71e01ddfded"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import os\n",
    "from phase_performance_hybrids.resnet50_cswin.model_v2 import ResNetCSWinHybrid\n",
    "# or use from phase_performance_hybrids.resnet50_cswin.model_v3 import ResNetCSWinHybridV3 as ReNsetCSWinHybrid for the parallel model\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "\n",
    "        pt = torch.exp(-ce_loss)\n",
    "\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    data_transforms = {\n",
    "        'train': v2.Compose([\n",
    "            v2.Resize((224, 224)),\n",
    "\n",
    "            # ------------------------------------ BASIC AUGMENTATION\n",
    "            v2.RandomHorizontalFlip(),\n",
    "            v2.RandomVerticalFlip(),\n",
    "            v2.ToTensor(),\n",
    "            # v2.Normalize([0.7553, 0.3109, 0.1059], [0.1774, 0.1262, 0.0863]),\n",
    "            v2.Normalize([0.7083, 0.2776, 0.0762], [0.1704, 0.1296, 0.0815]),\n",
    "            # ------------------------------------ BASIC AUGMENTATION\n",
    "\n",
    "            # ------------------------------------ HEAVY AUGMENTATION\n",
    "\n",
    "            # # Geometric Transforms\n",
    "            # v2.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "            # v2.RandomRotation(degrees=15),\n",
    "            # # Slight zoom/shift\n",
    "            # v2.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "\n",
    "            # # Color/Signal Transforms\n",
    "            # v2.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "\n",
    "            # # Noise & Robustness\n",
    "            # # Gaussian Blur helps ignore grain/noise\n",
    "            # v2.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "\n",
    "            # v2.ToTensor(),\n",
    "\n",
    "            # use this norm below when training on 2022\n",
    "            # # v2.Normalize([0.7083, 0.2776, 0.0762], [0.1704, 0.1296, 0.0815]),\n",
    "\n",
    "            # use this norm below when training on 2019\n",
    "            # v2.Normalize([0.7553, 0.3109, 0.1059], [0.1774, 0.1262, 0.0863]),\n",
    "\n",
    "            # # Occlusion (The Precision Booster)\n",
    "            # v2.RandomErasing(p=0.3, scale=(0.02, 0.15), ratio=(0.3, 3.3)),\n",
    "            # --------------------------------- HEAVY AUGMENTATION\n",
    "        ]),\n",
    "        'test': v2.Compose([\n",
    "            v2.Resize((224, 224)),\n",
    "            v2.ToTensor(),\n",
    "            # use this norm below when training on 2022\n",
    "            v2.Normalize([0.7083, 0.2776, 0.0762], [0.1704, 0.1296, 0.0815])\n",
    "\n",
    "            # use this norm below when training on 2019\n",
    "            # v2.Normalize([0.7553, 0.3109, 0.1059], [0.1774, 0.1262, 0.0863])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # use this below when training on 2019\n",
    "    # data_dir = '../cross_year_configurations_data/PlantVillage_1_2019train_2022test'\n",
    "\n",
    "    # use this below when training on 2022\n",
    "    data_dir = '../cross_year_configurations_data/PlantVillage_2_2022train_2019test'\n",
    "\n",
    "    dsets = {split: datasets.ImageFolder(os.path.join(data_dir, split), data_transforms[split])\n",
    "             for split in ['train', 'test']}\n",
    "\n",
    "    dset_loaders = {\n",
    "        'train': torch.utils.data.DataLoader(dsets['train'], batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "        'test' : torch.utils.data.DataLoader(dsets['test'],  batch_size=batch_size, shuffle=False, num_workers=4),\n",
    "    }\n",
    "\n",
    "    return dset_loaders['train'], dset_loaders['test']\n",
    "\n",
    "\n",
    "\n",
    "def measure_sparsity(model):\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            if hasattr(module, \"weight\"):\n",
    "                w = module.weight.data\n",
    "                total_params += w.numel()\n",
    "                zero_params += torch.sum(w == 0).item()\n",
    "    print(f\"Global Sparsity: {100. * zero_params / total_params:.2f}%\")\n",
    "    return zero_params / total_params\n",
    "\n",
    "\n",
    "def simple_evaluate(model, loader, device, threshold):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            preds = (probs >= threshold).long()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    return acc, f1\n",
    "\n",
    "def get_pruning_rate_per_step(final_target, k_steps):\n",
    "    if final_target <= 0: return 0.0\n",
    "    return 1 - (1 - final_target) ** (1 / k_steps)\n",
    "\n",
    "# for model_v2 (sequential)\n",
    "def apply_structured_pruning_step(model, rate_cnn, rate_trans):\n",
    "    # applies one round of structured pruning using L2 norm\n",
    "    count = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            # Prune output filters (dim=0) using L2 norm\n",
    "            prune.ln_structured(module, name='weight', amount=rate_cnn, n=2, dim=0)\n",
    "            count += 1\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            # Prune output neurons (dim=0)\n",
    "            if \"stage3\" in name or \"stage4\" in name:\n",
    "                prune.ln_structured(module, name='weight', amount=rate_trans, n=2, dim=0)\n",
    "            else:\n",
    "                prune.ln_structured(module, name='weight', amount=rate_cnn, n=2, dim=0)\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# for model_v3 (parallel)\n",
    "# def apply_structured_pruning_step(model, rate_cnn, rate_trans):\n",
    "#     count = 0\n",
    "#     for name, module in model.named_modules():\n",
    "\n",
    "#         if \"head\" in name:\n",
    "#             continue\n",
    "#         if isinstance(module, nn.Conv2d):\n",
    "#             prune.ln_structured(module, name='weight', amount=rate_cnn, n=2, dim=0)\n",
    "#             count += 1\n",
    "#         elif isinstance(module, nn.Linear):\n",
    "#             is_transformer_part = any(k in name for k in [\n",
    "#                 \"stage\",        # CSWin Stages 1-4\n",
    "#                 \"merge\",        # CSWin Merges\n",
    "#                 \"cross_attn\",   # The Cross Attention in Fusion\n",
    "#                 \"fusion\",       # The FFN in Fusion\n",
    "#                 \"cswin_proj\",   # Projections\n",
    "#                 \"fused_to\"      # Projections\n",
    "#             ])\n",
    "\n",
    "#             if is_transformer_part:\n",
    "#                 prune.ln_structured(module, name='weight', amount=rate_trans, n=2, dim=0)\n",
    "#             else:\n",
    "#                 # Treat everything else (e.g. Bridge projections) as CNN\n",
    "#                 prune.ln_structured(module, name='weight', amount=rate_cnn, n=2, dim=0)\n",
    "#             count += 1\n",
    "\n",
    "#     return count\n",
    "\n",
    "def make_pruning_permanent(model):\n",
    "    # burn mask into the weights so the next step treats the zeros as non-existent\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            if prune.is_pruned(module):\n",
    "                prune.remove(module, 'weight')\n",
    "\n",
    "def fine_tune_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "batch_size = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trainloader, testloader = load_data()\n",
    "\n",
    "model = ResNetCSWinHybrid(num_classes=2, resnet_pretrained=True, cswin_pretrained=True)\n",
    "path = '../phase_performance_hybrids/results_model_saves_resnet50_cswin/results_from_model_v2_heavy_augmentation/threshold_0.27_hybridv2_Tr2022_Te2019.pth'\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "FINAL_CNN_TARGET = 0.15\n",
    "FINAL_TRANS_TARGET = 0.17\n",
    "STEPS = 6\n",
    "EPOCHS_PER_STEP = 10\n",
    "THRESHOLD = 0.07\n",
    "\n",
    "cnn_step_rate = get_pruning_rate_per_step(FINAL_CNN_TARGET, STEPS)\n",
    "trans_step_rate = get_pruning_rate_per_step(FINAL_TRANS_TARGET, STEPS)\n",
    "\n",
    "print(f\"Goal: {FINAL_CNN_TARGET*100}% resnet, {FINAL_TRANS_TARGET*100}% trans\")\n",
    "print(f\"Schedule: {STEPS} steps, {EPOCHS_PER_STEP} epochs/step\")\n",
    "print(f\"Per step pruning rate - resnet: {cnn_step_rate:.4f}, trans: {trans_step_rate:.4f}\")\n",
    "\n",
    "acc_base, f1_base = simple_evaluate(model, testloader, device, THRESHOLD)\n",
    "print(f\"Baseline before acc: {acc_base:.4f}, F1: {f1_base:.4f}\\n\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) # low LR for gentle fine-tuning\n",
    "criterion = FocalLoss(gamma=2.0).to(device) # use same loss function during initial training\n",
    "\n",
    "for step in range(1, STEPS + 1):\n",
    "    print(f\"Step {step}/{STEPS}\")\n",
    "\n",
    "    # prune\n",
    "    print(\"Applying pruning\")\n",
    "    apply_structured_pruning_step(model, cnn_step_rate, trans_step_rate)\n",
    "    measure_sparsity(model)\n",
    "\n",
    "    # rehab\n",
    "    print(f\"Finetuning for {EPOCHS_PER_STEP} epochs\")\n",
    "    for epoch in range(EPOCHS_PER_STEP):\n",
    "        loss = fine_tune_epoch(model, trainloader, optimizer, criterion, device)\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f\"   Epoch {epoch+1}/{EPOCHS_PER_STEP} | Loss: {loss:.4f}\")\n",
    "\n",
    "    # check status\n",
    "    acc, f1 = simple_evaluate(model, testloader, device, THRESHOLD)\n",
    "    print(f\"Step {step} result: acc: {acc:.4f} (drop: {(acc_base-acc)*100:.2f}%)\")\n",
    "\n",
    "make_pruning_permanent(model)\n",
    "acc_final, f1_final = simple_evaluate(model, testloader, device, THRESHOLD)\n",
    "print(f\"Final result: acc: {acc_final:.4f}, {f1_final:.4f}, (drop: {(acc_base-acc_final)*100:.2f}%, {(f1_base - f1_final)*100:.2f}%)\")\n",
    "\n",
    "# Final Save\n",
    "# measure_sparsity(model)\n",
    "# save_folder = 'model_saves'\n",
    "# os.makedirs(save_folder, exist_ok=True)\n",
    "# save_path = os.path.join(save_folder, 'threshold_0.07_hybrid_Tr2019_Te2022_new_optimised.pth')\n",
    "# torch.save(model.state_dict(), save_path)\n",
    "# print(f\"Model saved to: {save_path}\")"
   ],
   "id": "bc83715c5313097b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Dynamic Quantization (INT8)",
   "id": "55073f63253f3c29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.quantization\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size = os.path.getsize(\"temp.p\")\n",
    "    print(f\"Model: {label:<15} | Size: {size/1e6:.2f} MB\")\n",
    "    os.remove(\"temp.p\")\n",
    "    return size\n",
    "\n",
    "def measure_inference_speed(model, loader, device):\n",
    "    model.eval()\n",
    "    # Warmup\n",
    "    dummy_input, _ = next(iter(loader))\n",
    "    dummy_input = dummy_input.to(device)\n",
    "    for _ in range(10):\n",
    "        _ = model(dummy_input)\n",
    "\n",
    "    start = time.time()\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            _ = model(inputs)\n",
    "            count += inputs.size(0)\n",
    "            if count > 200: # Measure first 200 images only to save time\n",
    "                break\n",
    "    end = time.time()\n",
    "\n",
    "    latency = (end - start) / count * 1000 # ms per image\n",
    "    print(f\"Latency: {latency:.2f} ms/image\")\n",
    "    return latency\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = '../phase_efficiency/iterative_structured_pruning/train22_test19_new_model/threshold_0.27_hybridv2_heavyAug_Tr2022_Te2019_iterStructPrunOnly.pth'\n",
    "\n",
    "model = ResNetCSWinHybrid(num_classes=2, resnet_pretrained=False, cswin_pretrained=False)\n",
    "state_dict = torch.load(CHECKPOINT_PATH, map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "size_fp32 = print_size_of_model(model, \"Pruned FP32\")\n",
    "\n",
    "# Apply Dynamic Quantization (INT8)\n",
    "# Quantify Linear layers. Conv2d quantization usually requires 'Static' quantization\n",
    "# which is more complex, but let's try standard dynamic first as it's the easiest win for Transformers.\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model.cpu(),\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "print(\"\\nQuantization Results\")\n",
    "size_int8 = print_size_of_model(quantized_model, \"Quantized INT8\")\n",
    "\n",
    "reduction = (size_fp32 - size_int8) / size_fp32 * 100\n",
    "print(f\"Size Reduction: {reduction:.2f}%\")\n",
    "\n",
    "# 3. Save the Efficient Model\n",
    "save_folder = '../phase_efficiency/quantization_int8/train22_test19_new_model'\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "q_path = os.path.join(save_folder, 'threshold_0.27_hybridv2_heavyAug_Tr2022_Te2019_pruned_quantized.pth')\n",
    "torch.save(quantized_model.state_dict(), q_path)\n",
    "print(f\"Efficient model saved to: {q_path}\")"
   ],
   "id": "5770410815e19a98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Test performance on newly compressed model",
   "id": "ec043241a01bf906"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "def evaluate_quantized(model, loader, device='cpu'):\n",
    "    # MUST USE CPU due to pytorch\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to('cpu')\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            # USE YOUR OPTIMAL THRESHOLD HERE\n",
    "            # 0.27 for train 2022, test 2019\n",
    "            # 0.07 for train 2019, test 2022\n",
    "            preds = (probs >= 0.27).long()\n",
    "\n",
    "            all_preds.extend(preds.numpy())\n",
    "            all_targets.extend(targets.numpy())\n",
    "\n",
    "    end = time.time()\n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    print(f\"NEW Compressed Model Accuracy: {acc:.4f}\")\n",
    "    print(f\"NEW Compressed Model F1: {f1:.4f}\")\n",
    "    print(f\"Inference Time: {end - start:.2f} seconds\")\n",
    "\n",
    "print(\"\\nTesting Efficient Model\")\n",
    "evaluate_quantized(quantized_model, testloader)"
   ],
   "id": "ce37bf40bc4f165a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Knowledge Distillation\n",
    "## For loading a specific model as the teacher, please visit distillation_config.py , line 69."
   ],
   "id": "c0f21ab965c09571"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from phase_efficiency.knowledge_distillation.scripts.train_knowledge_distillation import train_with_distillation, load_data\n",
    "from phase_efficiency.knowledge_distillation.scripts.distillation_config import (\n",
    "    EFFICIENTNET_VARIANTS,\n",
    "    DISTILLATION_CONFIGS,\n",
    "    LR_CONFIGS,\n",
    "    TEACHER_MODELS,\n",
    "    DATA_CONFIGS\n",
    ")\n",
    "\n",
    "\n",
    "def run_single_experiment(\n",
    "    experiment_name,\n",
    "    teacher_checkpoint,\n",
    "    student_name,\n",
    "    trainloader,\n",
    "    testloader,\n",
    "    distillation_config,\n",
    "    lr_config\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a single distillation experiment with given configuration\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"EXPERIMENT: {experiment_name}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Student Model: {student_name}\")\n",
    "    print(f\"Teacher: {teacher_checkpoint}\")\n",
    "    print(f\"Temperature: {distillation_config['temperature']}\")\n",
    "    print(f\"Alpha: {distillation_config['alpha']}\")\n",
    "    print(f\"Epochs: {distillation_config['epochs']}\")\n",
    "    print(f\"Backbone LR: {lr_config['backbone_lr']}\")\n",
    "    print(f\"Head LR: {lr_config['head_lr']}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    try:\n",
    "        losses, trained_student = train_with_distillation(\n",
    "            teacher_checkpoint=teacher_checkpoint,\n",
    "            student_name=student_name,\n",
    "            trainloader=trainloader,\n",
    "            testloader=testloader,\n",
    "            num_classes=2,\n",
    "            epochs=distillation_config['epochs'],\n",
    "            temperature=distillation_config['temperature'],\n",
    "            alpha=distillation_config['alpha']\n",
    "        )\n",
    "\n",
    "        # Save experiment-specific model\n",
    "        save_folder = 'cswin_fpn_hybrid/model_saves/experiments'\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        model_save_path = os.path.join(save_folder, f'{experiment_name}_{student_name}.pth')\n",
    "        torch.save(trained_student.state_dict(), model_save_path)\n",
    "        print(f\"\\nExperiment model saved: {model_save_path}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Experiment {experiment_name} failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def run_student_comparison():\n",
    "    \"\"\"\n",
    "    Compare different EfficientNet variants as students\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENT SET: Student Model Comparison\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    trainloader, testloader = load_data()\n",
    "    teacher_checkpoint = TEACHER_MODELS['default']\n",
    "    distillation_config = DISTILLATION_CONFIGS['default']\n",
    "    lr_config = LR_CONFIGS['default']\n",
    "\n",
    "    for student_name in EFFICIENTNET_VARIANTS[:3]:  # Test first 3 variants\n",
    "        experiment_name = f\"student_comparison_{student_name}\"\n",
    "        run_single_experiment(\n",
    "            experiment_name=experiment_name,\n",
    "            teacher_checkpoint=teacher_checkpoint,\n",
    "            student_name=student_name,\n",
    "            trainloader=trainloader,\n",
    "            testloader=testloader,\n",
    "            distillation_config=distillation_config,\n",
    "            lr_config=lr_config\n",
    "        )\n",
    "\n",
    "\n",
    "def run_temperature_ablation():\n",
    "    \"\"\"\n",
    "    Ablation study on temperature parameter\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENT SET: Temperature Ablation Study\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    trainloader, testloader = load_data()\n",
    "    teacher_checkpoint = TEACHER_MODELS['default']\n",
    "    student_name = 'efficientnet_b0'\n",
    "    lr_config = LR_CONFIGS['default']\n",
    "\n",
    "    temperatures = [2.0, 4.0, 6.0, 8.0]\n",
    "    # alpha = 0.7\n",
    "    alpha = 0.3\n",
    "    # modify best alpha here after getting best one\n",
    "\n",
    "    for temp in temperatures:\n",
    "        experiment_name = f\"temp_ablation_T{temp}\"\n",
    "        distillation_config = {\n",
    "            'temperature': temp,\n",
    "            'alpha': alpha,\n",
    "            'epochs': 100,  # Shorter for ablation\n",
    "            'batch_size': 128,\n",
    "        }\n",
    "\n",
    "        run_single_experiment(\n",
    "            experiment_name=experiment_name,\n",
    "            teacher_checkpoint=teacher_checkpoint,\n",
    "            student_name=student_name,\n",
    "            trainloader=trainloader,\n",
    "            testloader=testloader,\n",
    "            distillation_config=distillation_config,\n",
    "            lr_config=lr_config\n",
    "        )\n",
    "\n",
    "\n",
    "def run_alpha_ablation():\n",
    "    \"\"\"\n",
    "    Ablation study on alpha parameter (distillation vs hard label weight)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENT SET: Alpha Ablation Study\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    trainloader, testloader = load_data()\n",
    "    teacher_checkpoint = TEACHER_MODELS['default']\n",
    "    student_name = 'efficientnet_b0'\n",
    "    lr_config = LR_CONFIGS['default']\n",
    "\n",
    "    alphas = [0.3, 0.5, 0.7, 0.9]\n",
    "    temperature = 4.0\n",
    "\n",
    "    for alpha in alphas:\n",
    "        experiment_name = f\"alpha_ablation_A{alpha}\"\n",
    "        distillation_config = {\n",
    "            'temperature': temperature,\n",
    "            'alpha': alpha,\n",
    "            'epochs': 100,  # Shorter for ablation\n",
    "            'batch_size': 128,\n",
    "        }\n",
    "\n",
    "        run_single_experiment(\n",
    "            experiment_name=experiment_name,\n",
    "            teacher_checkpoint=teacher_checkpoint,\n",
    "            student_name=student_name,\n",
    "            trainloader=trainloader,\n",
    "            testloader=testloader,\n",
    "            distillation_config=distillation_config,\n",
    "            lr_config=lr_config\n",
    "        )\n",
    "\n",
    "\n",
    "def run_preset_configs():\n",
    "    \"\"\"\n",
    "    Run all preset configurations from config file\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENT SET: Preset Configurations\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    trainloader, testloader = load_data()\n",
    "    teacher_checkpoint = TEACHER_MODELS['default']\n",
    "    student_name = 'efficientnet_b0'\n",
    "    lr_config = LR_CONFIGS['default']\n",
    "\n",
    "    for config_name, distillation_config in DISTILLATION_CONFIGS.items():\n",
    "        experiment_name = f\"preset_{config_name}\"\n",
    "\n",
    "        run_single_experiment(\n",
    "            experiment_name=experiment_name,\n",
    "            teacher_checkpoint=teacher_checkpoint,\n",
    "            student_name=student_name,\n",
    "            trainloader=trainloader,\n",
    "            testloader=testloader,\n",
    "            distillation_config=distillation_config,\n",
    "            lr_config=lr_config\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Knowledge Distillation Experiment Suite\")\n",
    "    print(\"Teacher: ResNetCSWinHybrid (new_model)\")\n",
    "    print(\"Student: EfficientNet variants\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # if args.experiment == 'student_comparison':\n",
    "    #     run_student_comparison()\n",
    "    # elif args.experiment == 'temperature':\n",
    "    #     run_temperature_ablation()\n",
    "    # elif args.experiment == 'alpha':\n",
    "    #     run_alpha_ablation()\n",
    "    # elif args.experiment == 'presets':\n",
    "    #     run_preset_configs()\n",
    "    # elif args.experiment == 'all':\n",
    "    #     run_student_comparison()\n",
    "    #     run_temperature_ablation()\n",
    "    #     run_alpha_ablation()\n",
    "    #     run_preset_configs()\n",
    "\n",
    "    run_temperature_ablation()\n",
    "    # run_alpha_ablation()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"All Experiments Complete!\")\n",
    "    print(\"=\"*80)"
   ],
   "id": "2ee136c4a7db42fd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
