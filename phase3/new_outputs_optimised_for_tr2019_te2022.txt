Cell 1.2.2
Experiments
Final result: acc: 0.8584, 0.9046, (drop: -1.08%, -1.64%) - 0.08, 0.09
Final result: - 0.05
Final result: -

Final used - 0.08, 0.09
Goal: 8.0% resnet, 9.0% trans
Schedule: 6 steps, 10 epochs/step
Per step pruning rate - resnet: 0.0138, trans: 0.0156
Baseline before acc: 0.8476, F1: 0.8882

Step 1/6
Applying pruning
Global Sparsity: 1.54%
Finetuning for 10 epochs
   Epoch 5/10 | Loss: 0.0761
   Epoch 10/10 | Loss: 0.0674
Step 1 result: acc: 0.8657 (drop: -1.81%)
Step 2/6
Applying pruning
Global Sparsity: 3.00%
Finetuning for 10 epochs
   Epoch 5/10 | Loss: 0.0711
   Epoch 10/10 | Loss: 0.0643
Step 2 result: acc: 0.8860 (drop: -3.83%)
Step 3/6
Applying pruning
Global Sparsity: 4.44%
Finetuning for 10 epochs
   Epoch 5/10 | Loss: 0.0638
   Epoch 10/10 | Loss: 0.0620
Step 3 result: acc: 0.8961 (drop: -4.85%)
Step 4/6
Applying pruning
Global Sparsity: 5.86%
Finetuning for 10 epochs
   Epoch 5/10 | Loss: 0.0651
   Epoch 10/10 | Loss: 0.0636
Step 4 result: acc: 0.8784 (drop: -3.07%)
Step 5/6
Applying pruning
Global Sparsity: 7.27%
Finetuning for 10 epochs
   Epoch 5/10 | Loss: 0.0605
   Epoch 10/10 | Loss: 0.0617
Step 5 result: acc: 0.8578 (drop: -1.01%)
Step 6/6
Applying pruning
Global Sparsity: 8.68%
Finetuning for 10 epochs
   Epoch 5/10 | Loss: 0.0622
   Epoch 10/10 | Loss: 0.0606
Step 6 result: acc: 0.8584 (drop: -1.08%)
Final result: acc: 0.8584, 0.9046, (drop: -1.08%, -1.64%)
Global Sparsity: 8.68%